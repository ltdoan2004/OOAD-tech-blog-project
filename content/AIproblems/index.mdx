---
title: "AI’s Trust Problem"
description: "As AI becomes more powerful, it faces a major trust problem. Consider 12 leading concerns: disinformation, safety and security, the black box problem, ethical concerns, bias, instability, hallucinations in LLMs, unknown unknowns, potential job losses and social inequalities, environmental impact, industry concentration, and state overreach. Each of these issues is complex — and not easy to solve. But there is one consistent approach to addressing the trust gap: training, empowering, and including humans to manage AI tools."
image: "/public/blogs/AIproblem1.jpg"
publishedAt: "2024-05-03"
updatedAt: "2024-05-03"
author: "Bhaskar Chakravorti"
isPublished: true
tags:
  - AI
---
With tens of billions invested in AI last year and leading players such as OpenAI looking for trillions more, the tech industry is racing to add to the pileup of generative AI models. The goal is to steadily demonstrate better performance and, in doing so, close the gap between what humans can do and what can be accomplished with AI.

There is another gulf, however, that ought to be given equal, if not higher, priority when thinking about these new tools and systems: the AI trust gap. This gap is closed when a person is willing to entrust a machine to do a job that otherwise would have been entrusted to qualified humans. It is essential to invest in analyzing this second, under-appreciated gap — and in what can be done about it — if AI is to be adopted widely.


> The AI trust gap can be understood as the sum of the persistent risks (both real and perceived) associated with AI; depending on the application, some risks are more critical. These cover both predictive machine learning and generative AI. According to the Federal Trade Commission, consumers are voicing concerns about AI, while businesses are worried about several near to long term issues. Consider 12 AI risks that are among the most commonly cited across both groups: <

- Disinformation  
- Safety and security  
- The black box problem  
- Ethical concerns  
- Bias  
- Instability


<Image
src="/blogs/AIproblem2.png"
 width="718"
  height="404"
  alt="Image"
  sizes="100vw"
/>


Taken together, the cumulative effect of these risks contribute to broad public skepticism and business concerns about AI deployment. This, in turn, deters adoption. For instance, radiologists [hesitate to embrace AI](https://pubs.rsna.org/doi/full/10.1148/ryai.2020200088) when the black box nature of the technology prevents a clear understanding of how the algorithm makes decisions on medical image segmentation, survival analysis, and prognosis. Ensuring a level of transparency on the algorithmic decision-making process is critical for radiologists to feel they are meeting their professional obligations responsibly — but that necessary transparency is still a long way off. And the black box problem is just one of many risks to worry about. Given similar issues across different application situations and industries, we should expect the AI trust gap to be permanent, even as we get better in reducing the risks.

This has three major implications. First, no matter how far we get in improving AI’s performance, AI’s adopters — users at home and in businesses, decision-makers in organizations, policymakers — must traverse a persistent trust gap. Second, companies need to invest in understanding the risks most responsible for the trust gap affecting their applications’ adoption and work to mitigate those risks. And third, pairing humans with AI will be the most essential risk-management tool, which means we shall always have a need for humans to steer us through the gap — and the humans need to be trained appropriately.

Consider the 12 risks. For each of them, there are four questions: How they undermine trust in AI? What are the options — industry-initiated or required by regulators — for mitigating or managing the risk? Why do the options offer at best a partial remedy that allows the risk to persist? What are the lessons learned and implications? Collectively, these help break down the AI trust gap, why it can be expected to persist, and what can be done about it.  

---
## Disinformation

Online disinformation isn’t new, but AI tools have supercharged it. [AI-aided deepfakes have accompanied elections](https://apnews.com/article/artificial-intelligence-elections-disinformation-chatgpt-bc283e7426402f0b4baa7df280a4c3fd) from Bangladesh (where an opposition leader was depicted in a bikini) to Moldova (where a fake clip of president supporting pro-Russian party circulated before the election), giving voters a reason to distrust essential information necessary for the functioning of democracies. As of late 2023, [85% of internet users](https://www.theguardian.com/technology/2023/nov/07/85-of-people-worry-about-online-disinformation-global-survey-finds) worried about their inability to spot fake content online — a serious problem given [2024’s major elections](https://time.com/6550920/world-elections-2024/) across the globe.

Social media companies are largely failing to address the threat, as most have severely cut back on the human content moderators that are the most successful defense against disinformation. The largest platform company, Meta, for example, [drastically reduced](https://www.cnbc.com/2023/05/26/tech-companies-are-laying-off-their-ethics-and-safety-teams-.html) content moderation teams, shelved a fact-checking tool that was in development, and [cancelled](https://www.ft.com/content/6c2ddd5b-1593-45c0-95b5-b35f3711f288) contracts with external content moderators as part of its [“year of efficiency”](https://www.cnbc.com/2023/02/01/metas-year-of-efficiency-everything-wall-street-needed-to-hear.html) in 2023. Now, the platform is dealing with a [flood of bizarre advertising-driven AI generated content](https://www.404media.co/facebooks-algorithm-is-boosting-ai-spam-that-links-to-ai-generated-ad-laden-click-farms/), a reminder that social media recommendation algorithms are yet another form of AI that can be manipulated. Meta’s retreats were mirrored at YouTube, which cut its content moderation team, and at X, with [even more drastic dismantling](https://www.forbes.com/sites/thomasbrewster/2024/01/10/elon-musk-fired-80-per-cent-of-twitter-x-engineers-working-on-trust-and-safety/?sh=564586c79b35). (While Tik Tok hasn’t experienced the same level of cutbacks in its content moderation teams, it has to defend itself against a different set of worries: concerns over [compromised security and privacy of user data].)(https://techxplore.com/news/2023-08-kenya-tiktok-content-moderation.html) The algorithmic/automated content moderation often offered in place of human moderation is far from adequate.

In the absence of company initiated mitigation, the responsibility falls to regulators, who are stepping in to compel companies to act. In the U.S., multiple states have introduced bills targeting elections-related disinformation and deepfakes. The White House has an executive order requiring the “watermarking,” or clear labeling, of AI-created content, which is also required by the EU’s recently-passed [AI regulation](https://digital-strategy.ec.europa.eu/en/policies/regulatory-framework-ai). Elsewhere, India’s government holds [social media companies accountable](https://www.ft.com/content/fc600196-f821-4397-92fc-364ed8d2608a) for content flagged as harmful that has not been taken down.

Such well-intentioned risk-management measures may have unintended consequences, however, as platforms may simply allocate limited moderation resources to markets with the most regulatory pressure rather than invest in more moderation. The U.S. or the EU will get an over-allocation at the expense of the rest of the world — particularly developing world countries where regulatory and commercial demands are lower — even though there are many more users in these locations. There’s evidence was already happening before recent cutbacks: The Wall Street Journal [uncovered](https://www.wsj.com/articles/facebook-drug-cartels-human-traffickers-response-is-weak-documents-11631812953) that in 2020, 87% of Facebook’s content moderation time was spent on posts in the U.S., despite the fact that 90% of Facebook users were non-U.S.

The lesson is that we have to accept that the harsh reality that disinformation will be hard to eliminate. Depending on where you are in the world, it might even increase in volume and — with the growing sophistication of AI-aided deepfakes — in the degree of deceptiveness. Human vigilance and education in [“digital hygiene”](https://www.who.int/news-room/spotlight/let-s-flatten-the-infodemic-curve) will be essential.


## Safety and security

AI safety and security risks are alarming. A recent survey found that 37.8% to 51.4% of AI experts believe there is at least a 10% chance of catastrophic scenarios, including human extinction. Even among optimists, 48% estimate a 5% probability. Less severe risks, such as AI being used in cyberattacks or being "jailbroken," are also viewed as likely by 2043.

Regulations play a crucial role in mitigating these risks. The [White House executive order](https://www.whitehouse.gov/) and [EU regulations](https://ec.europa.eu/) require generative AI models above certain risk thresholds to publish results from simulated "red-team" attacks to identify vulnerabilities. However, the effectiveness of these measures is uncertain, and concerns about "security theater" persist. Startups may struggle to conduct thorough testing, introducing new vulnerabilities.

Experts agree that AI safety and security risks cannot be fully eliminated in the foreseeable future. Thus, awareness and preparedness are vital, especially in critical sectors like national security and healthcare. It is essential to keep humans involved in decision-making processes, particularly in sensitive scenarios like nuclear negotiations, to ensure that crucial decisions remain under human control.

## The black box problem

Transparency is crucial for building trust in AI. This includes informing users when they interact with AI, explaining outputs, and providing understandable information to stakeholders. Regulations like the [EU AI](https://ec.europa.eu/digital-strategy/our-policies/eu-ai-act_en) Act aim to enforce transparency, but AI companies often prioritize competitive advantage and intellectual property, leading to a "black box" effect where outputs are unclear.

Open-source AI development promotes transparency, but its effectiveness is limited by the complexity of AI models and the ambiguity in what constitutes "open-source." For instance, Meta's[Llama 2](https://ai.meta.com/llama/) is less transparent than its predecessor, Llama 1, and scores only 54 out of 100 on the Stanford Center for Research on Foundation Models’ transparency score. While companies like IBM provide "factsheets" for tracking, self-disclosures are not ideal for building trust.

Regulations could require external audits of AI models, but effective implementation needs clear criteria and credible auditors. A recent Cornell study found New York's law on auditing automated employment tools for bias to be ineffective. The National Institute of Standards and Technology has an AI Risk Management Framework, but without certification or audit methods, it remains largely ineffective.

In conclusion, while transparency in AI is improving, the black box problem persists. Each application area must develop initiatives to enhance transparency, such as improving the "interpretability" of AI in radiology to support clinical practice and adoption.

## Ethical concerns

[Most users agree](https://www.ibm.com/thought-leadership/institute-business-value/en-us/report/ai-ethics-in-action) that it’s critical to ensure that algorithms go beyond mathematics and data and are paired with guidelines ensuring ethical principles — e.g., that they respect human rights and values, no matter what the mathematics suggests. There have been several attempts at getting AI developers to coalesce around universally accepted ethical criteria: the [Asilomar AI principles](https://futureoflife.org/open-letter/ai-principles/), for example, embrace “human values,” “liberty and privacy,” “common good” among other ideals in developing and using AI models. But there are three obstacles to such endeavors.

For one, ethical ideals aren’t universal. The two pre-dominant AI nations, U.S. and China, interpret “liberty and privacy” differently: free speech is paramount in the U.S., while in China, unmoderated speech conflicts with the “common good.”  Even within the U.S., with its bitter culture wars and polarization, pro-life and pro-choice groups differ on “human values.” Some want AI to be [anti-“woke,”](https://www.politico.com/news/2023/07/17/ai-musk-chatgpt-xai-00106672) while others want AI’s [decolonization.](https://hai.stanford.edu/news/movement-decolonize-ai-centering-dignity-over-dependency)

Second, apolitical trans-national bodies have limited powers. The UN has [ethical AI principles](https://unsceb.org/sites/default/files/2022-09/Principles%20for%20the%20Ethical%20Use%20of%20AI%20in%20the%20UN%20System_1.pdf) consistent with its charter and UNESCO has brought companies together to commit to [building more ethical AI.](https://international.la-croix.com/news/science-tech/a-more-ethical-ai/19136) Given that most of AI development happens in the private sector, the UN’s leverage is limited.

Third, AI companies’ organizational incentives exacerbate tensions between ethics and other considerations. For example, with a workforce generally [leaning left](https://www.bloomberg.com/opinion/articles/2022-04-29/why-does-america-s-tech-workforce-lean-left?sref=htOHjx5Y) politically, there’s a need for political diversity in ethical oversight. This is hard to do in practice: Google’s efforts to assemble an AI ethics advisory council [fell apart](https://www.the-coming-wave.com/) when employees objected to the appointment the president of the right-wing Heritage Foundation. The much-publicized [boardroom versus Sam Altman drama](https://www.reuters.com/technology/sam-altman-return-openais-board-information-reports-2024-03-08/) at OpenAI, the failed attempt to separate DeepMind from Google’s standard business structure after its acquisition, and [the implosion of Stability AI’s leadership](https://techcrunch.com/2024/03/22/stability-ai-ceo-resigns-because-youre-not-going-to-beat-centralized-ai-with-more-centralized-ai/) are also recurring reminders of the battle over priorities in the pioneering AI companies: repeatedly, commercial goals win over AI for “common good” ideals.

The lesson here is that ethical dilemmas are context-dependent and will be a permanent fixture of AI systems; they are especially critical if they give rise to exclusionary or dangerous decisions. Keeping humans, including those assembled as governance or oversight boards and teams of external watchdogs, in the loop will be essential.

## Bias concerns

Biases in AI stem from many sources: biased or limited training data, the limitations of the people involved in the training, and even the usage context. They can erode confidence in AI models when they appear in critical applications, say, when lenders are found to be more likely to [deny home loan](https://apnews.com/article/lifestyle-technology-business-race-and-ethnicity-mortgages-2d3d40d5751f933a88c1e17063657586) to people of color by an even higher percentage when AI is used for mortgage approvals. There are several mitigating actions that can be taken, such as [enforcing fairness constraints](https://www.mckinsey.com/featured-insights/artificial-intelligence/tackling-bias-in-artificial-intelligence-and-in-humans) on AI models, adding more diverse sources of data, training the AI developers in recognizing bias, diversifying the AI talent pool, using [tools and metrics to test for biases,](https://github.com/Trusted-AI/AIF360) etc.

Despite these corrective measures, AI may never be reliably bias-free for several reasons. For one, because AI tools are trained in closed environments and may encounter unfamiliar application environments, they can produce surprising biases due to their limited exposure to real-world data. Further, the processes for testing the presence of bias are difficult. Definitions of what constitutes bias and unfairness can vary widely with contexts as different as the West, China, India — the idea of “fairness,” for instance, lends itself to [21 different definitions,](https://fairmlbook.org/) making it difficult to reach consensus on when an outcome is considered truly unbiased. Even “unlearning” biases can be hazardous, as it could [introduce new unpredictable associations](https://aclanthology.org/2023.findings-acl.602/) learned by the AI model, making matters worse overall; Google’s and Meta’s production of faulty ahistorical images offers a stark example of such risks. Besides, AI models also risk [running out of new high-quality data](https://arxiv.org/pdf/2211.04325.pdf) to train on and neutralizing biases arising from limited/low-quality datasets.

The lesson here is that we must accept that AI models will be trained with limitations — of data or trainers themselves operating with human limits — and biases will be inevitable. It will be essential to apply human judgment and vigilance along with swift remedial action before they do damage.

## Instability concerns

In some contexts, AI decisions can change drastically when the input is changed slightly and not in a meaningful way, leading to mistakes and small-to-catastrophic differences in outcomes. For instance, autonomous vehicles can be trusted with many functions but at times they fail: say, when a [small obstruction on a stop sign](https://www.11alive.com/article/news/investigations/graffiti-road-signs-may-confuse-self-driving-cars/85-15b853ab-ec64-4cfa-8155-14922e6e61b2) causes an AI-assisted car to drive past it. While AI models are constantly being improved upon by adding training data, enhancing testing protocols, and continuous machine learning, academic research on “stability” of AI has found that beyond basic problems, it is [mathematically impossible to develop universally stable AI algorithms.](https://scitechdaily.com/ais-achilles-heel-new-research-pinpoints-fundamental-weaknesses/) This means we can never be sure of AI making sound decisions when there is even a tiny bit of noise in the input data.

The lesson here is that AI systems can be sensitive to small changes, which are inevitable in the real world beyond the training dataset. The presence of alert humans to do a manual correction or over-ride will be critical in these situations.

## Author

Bhaskar Chakravorti 
<Image
src="/blogs/AIproblem3.gif"
 width="200"
  height="404"
  alt="Image"
  sizes="100vw"
/> is [the Dean of Global Business](https://fletcher.tufts.edu/people/bhaskar-chakravorti) at The Fletcher School at Tufts University and founding Executive Director of Fletcher’s [Institute for Business in the Global Context.](http://fletcher.tufts.edu/IBGC) He is the author of [The Slow Pace of Fast Change.](http://hbr.org/product/the-slow-pace-of-fast-change-bringing-innovations-/an/780X-SRN-ENG)



## Read more

On [AI and machine learning](https://hbr.org/topic/subject/ai-and-machine-learning?ab=articlepage-topic) or related topics [Business ethics,](https://hbr.org/topic/subject/business-ethics?ab=articlepage-topic) [Technology and analytics](https://hbr.org/topic/subject/technology-and-analytics?ab=articlepage-topic) and [Generative AI](https://hbr.org/topic/subject/generative-ai?ab=articlepage-topic)